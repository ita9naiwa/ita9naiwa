---
layout: "post"
title: "오랜만에 일기"
category: "recommender systems"
tag: "rlrP"
date: "2017-12-13 22:12"
---

일하면서 너무 멍청해진 것 같다. 놀기만 하고 일도 하나도 안하면서, 공부 또한 안하니 배운 걸 다 다시 까먹는다.
그래도 고딩들 수능 문제는 잘만 풀어대는 걸 보면 많이 연습한건 잘 까먹지 않는 경향이 있는 것 같은데, 기계학습을 할거야! 하고 통계니 뭐니 했던 건 죄다 란스가 다음 시리즈 나올때마다 레벨 떡락하는 느낌으로 까먹어버린 것 같다.
애초에 아는게 하나도 없었는데 뭔가 배운 느낌만 들었던 거고, 왠지 난 다시 아무것도 모른다는 사실을 기억해낸 걸 지도 모르겠다.


EM 알고리즘은 우리가 본 적이 없는 데이터가 있는 데이터셋에 대한 MLE를 Approximate하는 방법 중 하나이다.
본 적이 없는 데이터는 모르니까 그럴싸한 값을 집어넣고(E step), 본 적이 없는 데이터를 그럴싸한 값으로 채웠으면 missing data가 없어진 셈이니 이 data에 대해 likelihood를 maximize하는 paramter를 찾는다(M step).
이렇게 찾은 parameter를 이용해 모르는 값을 그럴싸한 값으로 채우고(E step) 다시 likelihood를 maximize(M step)을 반복하는 과정이다.
아마, 다른 분야에서는 다른 목적으로 EM을 사용할지도 모르겠는데, 기본적으로 통계적 모델링에서 EM을 사용하면 대체로 이런 느낌으로 사용한다고 한다.
설명만 보면 당연히 이해가 되지 않을 것이 분명하고, 마침 EM 알고리즘을 설명하기에 짱짱킹 좋은 예시(통계학입문 교재에서)를 찾았으므로 이를 기반으로 설명을 하면 좋을 것 같다.


$$
R  \simeq UV^T \text{, where }R_{ui} \neq 0
$$
