---
layout: article
title: "Combinatorial Optimization with Reinforcement Learning #1"
category: "Machine Learning"
tag: "Study"
mathjax: true
---

[Pointer Networks](https://arxiv.org/abs/1506.03134),
[Neural Combinatorial Optimization with Reinforcement Learning](https://arxiv.org/abs/1611.09940)
두 논문에 관한 리뷰입니다.

#### 잡담
개인적으로 일반적인 상황에서, 딥러닝이 그렇게 좋은지 잘 모르겠습니다. 다른 패턴 인식, 혹은 기계 학습 방법과 비교해서 (1) 어어어어으으어어어음청나게 Computation과 Memory를 많이 요구하고, (2) 일반적인 Tabular, Categorical Data를 다룰 때는 성능이 더 좋지도 않고(경험상 오히려 더 나쁜 편인 것 같아요.)
다만, 딥 러닝이 유난히! 엄청! 잘하는 일이 있는데, 비정형 데이터를 다루는 일인 것 같습니다'ㅅ'... Image나 Text Sequence를 잘 Handle한다는 얘기는 이제 사실 놀랍지도 않습니다. 이 논문도 Sequence를 다룹니다. 요지는 다음과 같습니다. input sequence를 정렬(논문엔 사실 없음), input sequence가 point라고 가정하면 TSP를 찾는다던가 하는, 조합론적인 문제(꽤나 풀기 어려운)를 풀 수 있다고 합니다. (이를 조금 응용하면 discrete한 문제(예를 들어, decision making)과 같은 분야에도 활용할 수 있을 것 같아서, 읽어보기로 결심했어용.)

## Pointer Networks
![enter image description here](https://lh3.googleusercontent.com/32fGbBX91XeRrjtqvAg3yo7xncJ4AaX8iArLnIryHJqH1iSHeqpM-bTQpCCOMcfnOYgUgdKUbHd4)

$I_i$ is a point(or, embedding of a point), (as in the paper) but I think any other data (embedded) can be fed to Encoder.

## Components of the Network

### Input/Output

Order를 갖거나, 혹은 갖지 않았거나 할 수 있지만, **특정한 목적을 달성하는** I의 subsequence 혹은 subset

Input: A sequence of points $\{I_1, I_2, ..., I_n\}$
Output: A  subset of inputs$\{I_{r(0)}, I_{r(1)}, ... I_{r(k)} \}$

#### Example
1.
		Input: 어떤 숫자의 Sequence [3, 1, 2, 5, 4]
		Output: 입력 Sequence의 역순 정렬 [5 ,4 , 3, 2, 1]
		(구현 부분에서 이 예시를 사용합니당.)
2.
		Input: 2D space 내에서의 점들의 집합
		Output: TSP path of input


### Encoder
Structure: RNN with LSTM Layer
Input: A sequence of points. point := $(x, y)$ and $x, y \in (0,1)$
Output: Encoding or Summarization of input sequecne such that ${e_1, e_2, ..., e_n }$, where $e_k$ is summarization of information of ${x_1, ..., x_k}$.

### Pointing Mechanism
논문의 핵심 부분인 것 같아용. 사실 이 논문에서, 이 부분을 제외하면 나머지 부분은 그냥 seq-to-seq 모델입니당;

input sequence가 Encoder에 의해 처리되고 나면,   encoding $\{e_1, e_2, ... e_n\}$을 갖게 된다.
$e_k$는 $I_k$의 embedded representation이라 생각하자.(사실 다를 지도 모르겠어용)
위에 Input/Output 부분에 나와있듯, Output을 만들기 위해, 우리는 다음 output으로 고를 input을 선택해야 하고, 논문에서는 **Pointing Mechanisim**이란 방법으로 이를 선택한다. 그리고, 이번에 고른input($j$번째 아웃풋)은 다음 $j+1$번째 입력을 고르기 위한 input으로 사용된다. (pointing mechanism이 다른 Attention mechanism과 비슷하다고 하지만, 사실 내가 Attention Mechanism을 잘 모르겠다.)

즉, **Pointeing mechanism**(이하 pointer)은 이번 스텝 k에서, 어떤 input I에 관심을 가져야 하는지 가리키는(그래서 이름이 pointer인거 같다) 방법이다.



#### Pointer Input/output
Input:
1. Encoding $\{e_1, e_2, ...e_k\}$ where $e_k \in R^h$ (h차원 벡터)
2. Current Decoding $d_k$ where $d_k \in  R^h$

#### Parameters
1. $W_e \in R^{p \times h}$
2. $W_d \in R^{p \times h}$
3. $v \in R^p$

#### Model Structrues
$u_{k, i} :=$ i'th element of pointer at step k $= v^T\text{tanh}(W_e e_i + W_dd_k)$
$o_k := \text{softmax}(u_k)$
next input that is attended := $I_{argmax_i(o_k)}$
$argmax(o_k)$ is the point chosen.
$I_{argmax_i(o_k)}$ is fed to next step $k+1$.

### Decoder
Input:  Sequence generated by Pointing Mechanism
Output1: $O = [o_1, o_2... o_k]$
1번째 인풋은, 아까 Input/Output에서 얘기했던, Input의 Subsequence를 얻기 위한 indices이다.
즉, $[I_{o_1}, ... I_{o_k}]$가 우리가 원하는 정답 시퀀스가 된다.

Output2: $O= [Pr(j|k=0), Pr(j|k=1), ..., ]$
Output 의 k, j번째 항에는 k번째 스텝에서 input j가 선택될 확률을 담고 있다.

### Supervised Learning.
정답 Sequence를 알고 있다면, Negative Log Likelihood와 같은 Measure로 정답 Sequence와 현재 확률 분포간의 차이를 이용해 Network를 train할 수 있다.

### Policy gradient
[Neural Combinatorial Optimization with Reinforcement Learning](https://arxiv.org/abs/1611.09940)에서 위의 네트워크를 Policy Gradient를 이용해 training하는 방법을 제안했다. Output 의 k, j번째 항에는 k번째 스텝에서 input j가 선택될 확률을 담고 있다.
$$
O_{j,k} = P[o(j) == k |  o(0), o(1), ...o(j-1), \theta]
$$

즉, step k에서 input j를 골라가면서, 전부 곱하면 다음이 됨을 알 수 있다.
$$
	\prod_{(j,k) \in o}^{n}O_{j,k} = P(\pi|\theta)
$$

위 네트워크를 state(Input sequence)에 대해, action을 만드는 과정으로 생각해보자. 그렇다면, Trajectory $o=\pi$를 생각할 수 있고, input을 state s라 생각할 수 있다. State value $s$에 대해 식을 다시 적으면
$$
	\prod_{(j,k) \in \pi}^{n}O_{j,k} = P(\pi|\theta, s)
$$
이며, Policy Gradient Theorem에 의해 이를 Supervised Learning을 사용하지 않더라도, update할 수 있다.
$$
J(\theta)=E_{\pi}[(V(s) - b(s))\nabla_\theta \log(P(\pi|s))] = \\
E_{\pi}[(V(s) - b(s))\nabla_\theta \sum_{j, k}\log(O_{j,k})]
$$