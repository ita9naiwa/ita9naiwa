---
layout: article
title: "TSP 문제를 강화 학습으로 풀기"
category: "Machine Learning"
tag: "Study"
mathjax: true
---

### [Neural Combinatorial Optimization with Reinforcement Learning](https://arxiv.org/abs/1611.09940)
[Attention, Learn to Solve Routing Problems!](https://openreview.net/forum?id=ByxBFsRqYm) 논문 리뷰입니다.

### 잡담
의외로 많은 문제들이, 복잡한 강화학습 기술을 사용하지 않고, 단지 두 가지 테크닉을 (엄청 잘 했지만) 이용해 새로운 도메인에서의 문제를 멋지게 해결해내는 경구가 많습니다. 두가지 테크닉은 다음과 같아요.

1. Neural Network을 이용한 Function Approximator를 만든다.
2. Policy Gradient(REINFORCE Algorithm)만으로 대단한 일을 하는 경우가 많습니다.

저도 이런 논문을 열심히 읽어서, 기계 학습을 일견 전혀 상관없어 보이는 분야에 적용해 멋진 일을 하고 싶습니다.


### Traveling Salesperson Problem

![TSP](http://mathworld.wolfram.com/images/eps-gif/TravelingSalesmanProblem_1000.gif)

Traveling Salesperson Problem은 조합론, 컴퓨터 과학에서 굉장히 널리 알려진 문제이다. 점들의 집합이 주어져 있을 때, 모든 점을 순회하고 자기자신으로 돌아오는 경로 $\pi$ 중 길이가 짧은 $\pi$를 찾는 문제이다. 조금 더 자세한 정의는 [TSP 위키피디아 설명](https://en.wikipedia.org/wiki/Travelling_salesman_problem)를 참고하면 좋을 것 같다.

이 문제는 어려운 걸로 유명하다! 정확한 해를 구하는 것은 다항시간 내에 불가능하다는 것이 이미 알려져 있고, 그래서 충분히 좋은 근사치를 계산하는 알고리즘이 많이 나와 있다. 이 논문도, 그런 근사치를 구하는 알고리즘 + 1일 뿐인데.... 다만, 다른 점은, **강화 학습을 이용하는 풀이를 제안했다는 점이다!** 이게 얼마나 대단한 일이냐면, 강화 학습을 이용한 방법은, **문제에 대한 사전지식이 사실상 없다는 점이다!** 다른 복잡하거나 어려워서, 휴리스틱을 제안하기 힘든 문제에 대해서도 강화학습이 충분히 좋은 Solution을 줄 수 있다는 점을 보였다는 점이 이러한 시도를 대단하게 만드는 것 같다.

### 제안 방법

제안된 논문은 Attention mechanism을 많이 이용한다. 따라서, 어텐션에 대해 간단히 설명하는게 좋을 것 같아 설명...

#### 사용된 Sub-modules

##### dot-product Attention

> 수식적이 어떻게 돌아가는진 알겠는데, 어텐션이 어떤 의미를 지니는 지는 솔직히 도무지 모르겠습니다... 아시는 분 있으면 찾아가서 이게 무슨 의미인지 물어보고 싶어요..

입력: query vector $p$, sequence of key vectors $h= [h_1, \cdots , h_n]$
Trainable Parameters: $W_q \in d_h \times d_q , W_k \in d_h \times d_k , W_v \in d_v \times d_k$

우선
$q = W_q p$, $k_i = W_k h_i$, $v_i = W_v h_i$ 으로 정의한다.

$$
	v_i = W_v h_i
$$

$$
	l_j = \frac{q^Tk_i}{\sqrt{d_k}}
$$

그럼 l은 길이가 n인 vector가 된다. 얘를 normalize해서 확률 분포, 혹은 Attention Weight으로 변환하고 싶다. 그래서 소프트맥스를 적용한다.

$$
	\alpha_i = \frac{\exp{(l_j)}}{\sum_{j'} \exp{(l_{j'})}}
$$

$$
	h' = \sum_j \alpha_j v_j
$$
이 과정을 $\text{Attention}(p, H)$라 합니다. 어텐션에 대한 해석은 솔직히 잘 모르겠습니다...

##### Multi-head attention
"Attend to"를 계산할 때, 여러 작은 어텐션의 합으로 어텐션을 계산한 후, 이를 Aggregate하는 것이 좋다고 알려져 있다. 이는 다음과 같이 구현할 수 있다.

$$
	h_1'= \text{Attention}_1(p, H) \\
	h_2' = \text{Attention}_2(p, H) \\
	\cdots \\
	h_k' = \text{Attention}_k(p, H)
$$

$k$개의 어텐션을 구한 이후, 이를 합친다.
$$
	\text{MHA}(p, H) = \sum_i W_h h_i
$$

##### Self-Attention

$$
	h_1'= \text{Attention}(h_1, H) \\
	h_2' = \text{Attention}(h_2, H) \\
	\cdots \\
	h_n' = \text{Attention}(h_n, H)
$$
이렇게 n개의 어텐션을 구한다면, 각각 key 자기 자신에 대한 어텐션이 n개 구해지게 되는 것이다. 이를 Self-Attention이라 하며, 입력과 출력의 모양은 동일하다.

##### Multi-Head Self-Attention
$$
	h_1'= \text{MHA}(h_1, H) \\
	h_2' = \text{MHA}(h_2, H) \\
	\cdots \\
	h_n' = \text{MHA}(h_n, H)
$$

Self-Attention의 Attention 부분을 Multi-head Attention으로 교체해주면 된다. 이후 모델에서 사용하는 모든 Self Attention은 multi head이므로, 이를 편의상 \text{SelfAttention}(H)이라 부르겠습니다.


#### Encoder
개별 점들을 Attention Mechanism을 이용해 개별 점들을 Encode하고, 이를 이용해 point들의 집합 전체를 표현하는 방법을 제안합니다. 이 방법은 [Transformer](https://arxiv.org/abs/1706.03762)와 상당히 유사합니다. Transformaer 모델에 대해 추가적으로 관심이 있으신 분들은 [좋은 포스트](https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225)를 참고하면 좋을 것 같아요.
위의 모듈들이 어떻게 구현되어 있는지 알고 있다면 인코더 부분은 굉장히 심플하게 이해할 수 있기 때문에, Pseudo code로 설명을 적는 것이 좋을 것 같아요.

##### initial point embedding
각각의 point는 TSP 문제의 경우 2차원이다. 이 2차원 벡터를 $d_h$차원 스페이스로 embedding하는데, 이는 간단한 affine projection으로 표현할 수 있다.
$$
	h_i^{(0)} = W^x x_i + b_x
$$

$$
	h_i^{\text{temp}} = h_i^{(l)} + \text{MHA}(h_i^{(l)}, [h_1^{(l)}, h_2^{(l}), \cdots, h_n^{(l)}])
$$
$$
	h^{(l+1)} = h^{\text{temp}} + \text{MLP}(h^{\text{temp}})
$$
$\text{MLP}$ 는 MLP 하나의 hidden layer를 가진 neural network이며, hidden layer의 activation은 relu이다.

이 과정을 l번 반복하면 개별 point들의 임베딩을 얻을 수 있다. 논문에서는 hidden size를 128로 정하고, self attention을 3번 반복하는 것이 계산 속도와 TSP 성능 두 측면에서 적절한 것 같다고 언급하고 있다.


이렇게 몇번의 레이어를 거쳐서 나온(논문에서는 세번) 결과를 각 점의 Embedding이라고 생각합니다.

또한, 단순히
$$
	\bar h^{(F)} = \frac{1}{n} \sum_i^n h_i^{(F)}
$$
F번 Self-attention을 거친 레이어의 개별 point의 평균을 집합 전체의 embedding으로서 사용하고 있다.

#### Decoder
디코더는 $n$번의 step으로 이루어져 있다. 한 스텝마다, 하나의 point를 리턴한다.
즉, 디코더는 point set $\{p_1, p_2, ..., p_n\}$에 대한 permutation $\pi$를 생성하는데, 이게 TSP의 traverse order와 동일하다.

Attention(Pointer)에 사용할 입력으로, timestep마다 다른 $p$를 만들어 사용한다. 사용하는 정보는 다음과 같다. (1) 그래프 임베딩 정보, (2) 가장 마지막에 방문한 point의 임베딩, (3) 가장 처음에 방문했던 point의 임베딩이다.

$$
	p = \begin{cases}
	\text{concat}(\bar h^{(F)}, h_{\pi_1}, h_{\pi_{t-1}}) & t > 1 \\
	\text{concat}(\bar h^{(F)}, e_1, e_f) & t = 1
	\end{cases}
$$

##### Glimpse
$$
	p' = MHA(p, [h_1^{(F)}, \cdots, h_n^{(F)}])
$$
확률 분포를 생성하기 위한 $p$를 조금 더 잘 표현하기 위해, 이를 다시 한 번 어텐션을 거치도록 만들어 준다.
다만, softmax를 만들 때 생성된 logit $l_i$에 조금 처리를 해 주는데, 이는 다음과 같다.

$$
l_j = \begin{cases}
  \frac{q^Tk_i}{\sqrt{d_k}} & \text{if }j \neq  \pi_{t'} \forall t' < t\\
   -\infty & \text{otherwise}
\end{cases}
$$
즉, 이미 방문했던 point는 Attention을 계산할 때 제외해준다는 점이다.

##### Pointer
[Pointer Networks](https://arxiv.org/abs/1506.03134) 논문에서 제안된, 어텐션의 Variant이다.

위에서, $Attention(p, H)$를 계산할 때 $\alpha$가 계산됩니다. 이 $\alpha$를 확률 분포로 해석한다. $\alpha_i$은 i번째 input이 선택될 확률로 해석합니다.
다만, logit을 계산하는 부분이 조금 다르다.

$$
l_j = \begin{cases}
  C \tanh\frac{q^Tk_i}{\sqrt{d_k}} & \text{if }j \neq  \pi_{t'} \forall t' < t\\
   -\infty & \text{otherwise}
\end{cases}
$$
we clip the sesult within $[-C, C]$ using $\tanh(\cdot)$

$$
	p_{\theta}(\pi_t = i \vert s, \pi_{1:t-1})= p_i = \alpha_j = \frac{\exp(l_j)}{\sum_{j'} \exp(l_{j'})}
$$
이 확률 벡터를 계산했으면 Policy Gradient를 통해 이를 계산할 수 있겠지...


다만, $\alpha_i$를 계산할 때, 위에서와 마찬가지로 이미 방문했던 point는 방문해야 하지 말아야 한다.

### 설명하지 않은(사실 못한) 것들...

사실 Combinatorial optimization을 다른 분야에 적용해보기 위해서 논문을 읽고 구현해보다가, 조금 더 자세히 기록해두고 싶어서, 중요한 포인트를 집어서 적었는데, 사실 논문에는 더 많은 내용이 있다. 관심이 가는 분들은 논문을 직접 읽어보면 정말 좋을 것 같다. 사실, 내 글을 읽고 원 논문에 관심이 생긴다면 기쁠 것 같다.

- TSP 문제가 아닌, 다른 여러 조합론 문제도 제안된 모델을 통해 해결할 수 있음을 보임
- 새로운 Greedy Rollout 방법을 제안함



### 구현체
논문에서 리뷰한 [Attention, Learn to Solve Routing Problems!](https://openreview.net/forum?id=ByxBFsRqYm)과 [Neural Combinatorial Optimization with Reinforcement Learning](https://arxiv.org/abs/1611.09940) 논문을 함께 구현해
[깃헙 링크](https://github.com/ita9naiwa/TSP-solver-using-reinforcement-learning)에 올려두었습니다.
